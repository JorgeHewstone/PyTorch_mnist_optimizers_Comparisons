
# Comparaci√≥n de Optimizadores en PyTorch con MNIST y FastAPI

[](https://pytorch.org/)
[](https://fastapi.tiangolo.com/)

Este proyecto tiene un doble objetivo:

1.  **Educativo:** Realizar un experimento para comparar la convergencia, velocidad y precisi√≥n de cinco algoritmos de optimizaci√≥n de PyTorch (SGD, Adam, RMSprop, Adagrad, AdamW) en la tarea de clasificaci√≥n de MNIST.
2.  **Pr√°ctico:** Desplegar los modelos entrenados en una aplicaci√≥n web interactiva usando FastAPI, donde el usuario puede dibujar un d√≠gito y recibir una predicci√≥n en tiempo real.

-----

## üî¨ Resultados del Experimento

El experimento entrena un modelo de red neuronal simple (MLP) en el dataset MNIST, una vez por cada optimizador, usando los mismos hiperpar√°metros de base. La visualizaci√≥n principal compara la ca√≠da de la funci√≥n de p√©rdida (Loss) a lo largo de los pasos de entrenamiento para cada optimizador.

**El gr√°fico generado por el script (`results/loss_comparison.png`) se mostrar√° aqu√≠:**

*(Esta imagen se genera autom√°ticamente en la carpeta `results/` al ejecutar `src/train.py`)*

-----

## üß† Fundamentos: Una Breve Mirada a los Optimizadores

Un **optimizador** es el algoritmo que ajusta los pesos (par√°metros) de la red neuronal para minimizar la funci√≥n de p√©rdida. Es el "motor" que impulsa el aprendizaje. Los gradientes (calculados por *backpropagation*) nos dicen la *direcci√≥n* del ascenso, y el optimizador decide *c√≥mo y cu√°nto* movernos en la direcci√≥n opuesta.

### 1\. SGD (Stochastic Gradient Descent)

  * **Qu√© es:** El algoritmo fundamental. Actualiza los pesos bas√°ndose √∫nicamente en el gradiente del lote actual.
  * **Concepto:** `peso = peso - (learning_rate * gradiente)`
  * **Pros:** Simple, f√°cil de entender, computacionalmente ligero.
  * **Contras:** Puede ser ruidoso y lento para converger. Puede atascarse en m√≠nimos locales o puntos de silla.
  * **Variaci√≥n (la que usamos):** **SGD con Momentum** a√±ade una fracci√≥n del vector de actualizaci√≥n anterior, lo que ayuda a suavizar la trayectoria y acelerar la convergencia a trav√©s de "valles".

### 2\. Adagrad (Adaptive Gradient)

  * **Qu√© es:** Un optimizador adaptativo. Mantiene *learning rates* separados para cada par√°metro y los adapta bas√°ndose en los gradientes pasados.
  * **Concepto:** Da "pasos" m√°s peque√±os para par√°metros que han recibido gradientes grandes y frecuentes (se "cansa" r√°pido).
  * **Pros:** Excelente para datos dispersos (sparse data), ya que presta m√°s atenci√≥n a caracter√≠sticas raras.
  * **Contras:** Su *learning rate* global decae agresivamente y puede llegar a ser tan peque√±o que el entrenamiento se detiene prematuramente.

### 3\. RMSprop (Root Mean Square Propagation)

  * **Qu√© es:** La soluci√≥n al problema de Adagrad. En lugar de sumar *todos* los gradientes cuadrados pasados, utiliza un **promedio m√≥vil exponencial**.
  * **Concepto:** Mantiene la adaptabilidad por par√°metro, pero evita que el *learning rate* muera tan r√°pido.
  * **Pros:** Convergencia r√°pida y estable en muchos problemas.
  * **Contras:** Puede ser sensible a la elecci√≥n del *learning rate* inicial.

### 4\. Adam (Adaptive Moment Estimation)

  * **Qu√© es:** El est√°ndar de facto actual en *Deep Learning*. Combina lo mejor de dos mundos: **Momentum** (primer momento, la "velocidad") y **RMSprop** (segundo momento, la "adaptabilidad" del *learning rate*).
  * **Concepto:** Mantiene un promedio m√≥vil tanto del gradiente como de su cuadrado.
  * **Pros:** Generalmente converge m√°s r√°pido que otros m√©todos y es menos sensible a la elecci√≥n de hiperpar√°metros.
  * **Contras:** Requiere m√°s memoria para almacenar sus "momentos" por cada par√°metro.

### 5\. AdamW (Adam with Weight Decay)

  * **Qu√© es:** Una correcci√≥n a Adam. La implementaci√≥n original de Adam mezclaba la regularizaci√≥n L2 (*weight decay*) con la actualizaci√≥n del gradiente, lo cual no es √≥ptimo para optimizadores adaptativos.
  * **Concepto:** Desacopla el *weight decay* de la actualizaci√≥n adaptativa, aplic√°ndolo directamente al peso al final del paso.
  * **Pros:** A menudo conduce a una mejor generalizaci√≥n (mejor rendimiento en el conjunto de prueba) que el Adam est√°ndar.

-----

## üìÅ Estructura del Proyecto

```
/
‚îú‚îÄ‚îÄ app/                  # C√≥digo del backend (FastAPI) y frontend
‚îÇ   ‚îú‚îÄ‚îÄ static/js/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ drawing.js    # L√≥gica del canvas
‚îÇ   ‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index.html    # Frontend HTML
‚îÇ   ‚îî‚îÄ‚îÄ main.py           # Servidor FastAPI y l√≥gica de predicci√≥n
‚îú‚îÄ‚îÄ src/                  # C√≥digo de Machine Learning
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py    # Funciones para cargar MNIST
‚îÇ   ‚îú‚îÄ‚îÄ model.py          # Definici√≥n de la clase SimpleNN
‚îÇ   ‚îî‚îÄ‚îÄ train.py          # Script principal para entrenar y comparar
‚îú‚îÄ‚îÄ models/               # (Ignorado por Git) Aqu√≠ se guardan los .pth
‚îú‚îÄ‚îÄ results/              # (Ignorado por Git) Aqu√≠ se guardan los gr√°ficos
‚îú‚îÄ‚îÄ .gitignore            # Ignora modelos, datos y caches
‚îú‚îÄ‚îÄ README.md             # ¬°Este archivo!
‚îî‚îÄ‚îÄ requirements.txt      # Dependencias de Python
```

-----

## üöÄ Instalaci√≥n y Uso

Sigue estos pasos para poner en marcha el proyecto.

### 1\. Preparar el Entorno

1.  **Clona el repositorio:**

    ```bash
    git clone https://github.com/TuUsuario/pytorch-mnist-optimizers.git
    cd pytorch-mnist-optimizers
    ```

2.  **(Recomendado) Crea un entorno virtual:**

    ```bash
    python -m venv venv
    # En Windows:
    .\venv\Scripts\activate
    # En macOS/Linux:
    source venv/bin/activate
    ```

3.  **Instala las dependencias:**

    ```bash
    pip install -r requirements.txt
    ```

### 2\. Entrenar los Modelos

Este es el paso m√°s importante. El script entrenar√° los 5 modelos secuencialmente y guardar√° los artefactos (`.pth`) y el gr√°fico de comparaci√≥n.

```bash
# Navega a la carpeta 'src'
cd src/

# Ejecuta el script de entrenamiento
python train.py
```

Al finalizar, deber√≠as tener 5 archivos `.pth` en la carpeta `models/` y un `loss_comparison.png` en la carpeta `results/`.

### 3\. Ejecutar la Aplicaci√≥n Web

Una vez entrenados los modelos, puedes iniciar el servidor FastAPI.

```bash
# Navega a la carpeta 'app' (desde la ra√≠z del proyecto)
cd app/

# Inicia el servidor
uvicorn main:app --reload
```

### 4\. Probar el Proyecto

Abre tu navegador web y ve a **`http://127.0.0.1:8000`**.

¬°Ahora puedes dibujar un d√≠gito, seleccionar uno de los modelos entrenados (basados en el optimizador) y ver la predicci√≥n en tiempo real\!
